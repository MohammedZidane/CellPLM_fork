{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fc5a70",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafcd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import hdf5plugin\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from scipy.sparse import csr_matrix\n",
    "from CellPLM.utils import set_seed\n",
    "from CellPLM.utils.data import stratified_sample_genes_by_sparsity\n",
    "from CellPLM.pipeline.ssl import SSLPipeline, SSLDefaultPipelineConfig, SSLDefaultModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d3952",
   "metadata": {},
   "source": [
    "## Specify important parameters before getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874f0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'Liver' # 'Lung'\n",
    "PRETRAIN_VERSION = '20231027_85M'\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a88ad",
   "metadata": {},
   "source": [
    "## Load Downstream Dataset\n",
    "\n",
    "The example datasets here are taken from `HumanLungCancerPatient2` from [Lung cancer 2](https://info.vizgen.com/ffpe-showcase?submissionGuid=88ba0a44-26e2-47a2-8ee4-9118b9811fbf), `GSE131907_Lung` from [GSE131907](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE131907), `HumanLiverCancerPatient2` from [Liver cancer 2](https://info.vizgen.com/ffpe-showcase?submissionGuid=88ba0a44-26e2-47a2-8ee4-9118b9811fbf) and `GSE151530_Liver` from [GSE151530](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE151530).\n",
    "\n",
    "The data we released are already preprocessed, where we subset the SRT dataset by selecting the first 100 FOVs. For scRNA-seq datasets, we only preserve genes that are overlapped with the SRT dataset. This is to ensure that for all the genes involved in this example, we know the ground-truth gene expressions from the SRT dataset. Later, we will hold out part of the genes from the SRT dataset, and leverage information from the scRNA-seq dataset to impute them. Therefore, this gene filtering is only for the convenience of evaluation. In practice, we can leverage the scRNA-seq dataset to impute unmeasured genes in the SRT dataset.\n",
    "\n",
    "After the preprocessing, the AnnData object must contain following information:\n",
    "\n",
    "* `.obs['platform']` A string label for identification of SRT data. When platform is set to 'cosmx' or 'merfish', spatial positional information will be loaded.\n",
    "* `.obs['x_FOV_px']` For SRT data, please store the float/int type X coordinate of each cell here.\n",
    "* `.obs['y_FOV_px']` For SRT data, please store the float/int type Y coordinate of each cell here.\n",
    "* `.obs['batch']` For SRT data, batch refers to an FOV. For scRNA-seq data, batch refers to a sample. Please store a string type batch identifier here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c4d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(11)\n",
    "if DATASET == 'Lung':\n",
    "    query_dataset = 'HumanLungCancerPatient2_filtered_ensg.h5ad'\n",
    "    ref_dataset = 'GSE131907_Lung_ensg.h5ad'\n",
    "    query_data = ad.read_h5ad(f'../data/{query_dataset}')\n",
    "    ref_data = ad.read_h5ad(f'../data/{ref_dataset}')\n",
    "\n",
    "elif DATASET == 'Liver':\n",
    "    query_dataset = 'HumanLiverCancerPatient2_filtered_ensg.h5ad'\n",
    "    ref_dataset = 'GSE151530_Liver_ensg.h5ad'\n",
    "    query_data = ad.read_h5ad(f'../data/{query_dataset}')\n",
    "    ref_data = ad.read_h5ad(f'../data/{ref_dataset}')\n",
    "\n",
    "target_genes = stratified_sample_genes_by_sparsity(query_data, seed=11) # This is for reproducing the hold-out gene lists in our paper\n",
    "query_data.obsm['truth'] = query_data[:, target_genes].X.toarray()\n",
    "query_data[:, target_genes].X = 0\n",
    "train_data = query_data.concatenate(ref_data, join='outer', batch_key=None, index_unique=None)\n",
    "\n",
    "train_data.obs['split'] = 'train'\n",
    "train_data.obs['split'][train_data.obs['batch']==query_data.obs['batch'][-1]] = 'valid'\n",
    "train_data.obs['split'][train_data.obs['batch']==ref_data.obs['batch'][-1]] = 'valid'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b1058",
   "metadata": {},
   "source": [
    "## Specify gene to impute\n",
    "In the last step, we merge the query dataset (SRT) and the reference dataset (scRNA-seq). However, the query dataset does not measures all the genes. For fine-tuning the model, we need to specify which genes are measured in each dataset. Therefore, we create a dictionary for the imputation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8a39a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_genes: ['ENSG00000041982', 'ENSG00000158296', 'ENSG00000081041', 'ENSG00000113318', 'ENSG00000165699', 'ENSG00000171223', 'ENSG00000110777', 'ENSG00000006210', 'ENSG00000197461', 'ENSG00000121594', 'ENSG00000174807', 'ENSG00000109072', 'ENSG00000174125', 'ENSG00000178999', 'ENSG00000181449', 'ENSG00000150045', 'ENSG00000184451', 'ENSG00000180644', 'ENSG00000105976', 'ENSG00000164867', 'ENSG00000114013', 'ENSG00000078098', 'ENSG00000187498', 'ENSG00000017427', 'ENSG00000103855', 'ENSG00000169248', 'ENSG00000204103', 'ENSG00000163508', 'ENSG00000171552', 'ENSG00000092969', 'ENSG00000163513', 'ENSG00000077150', 'ENSG00000122512', 'ENSG00000125657', 'ENSG00000169896', 'ENSG00000170458', 'ENSG00000121858', 'ENSG00000115414', 'ENSG00000177606', 'ENSG00000101017', 'ENSG00000203747', 'ENSG00000188676', 'ENSG00000138722', 'ENSG00000137673', 'ENSG00000186891', 'ENSG00000138755', 'ENSG00000111206', 'ENSG00000172216', 'ENSG00000111276', 'ENSG00000123384', 'ENSG00000211445', 'ENSG00000160683', 'ENSG00000187079', 'ENSG00000126456', 'ENSG00000116031', 'ENSG00000198851', 'ENSG00000079385', 'ENSG00000158481', 'ENSG00000144354', 'ENSG00000184371', 'ENSG00000143297', 'ENSG00000116209', 'ENSG00000100450', 'ENSG00000018408', 'ENSG00000068078', 'ENSG00000115415', 'ENSG00000162337', 'ENSG00000119888', 'ENSG00000069702', 'ENSG00000076003', 'ENSG00000260314', 'ENSG00000152256', 'ENSG00000143184', 'ENSG00000158859', 'ENSG00000128604', 'ENSG00000157764', 'ENSG00000168329', 'ENSG00000149554', 'ENSG00000112033', 'ENSG00000149294', 'ENSG00000135549', 'ENSG00000119630', 'ENSG00000109819', 'ENSG00000116062', 'ENSG00000100385', 'ENSG00000111537', 'ENSG00000171791', 'ENSG00000119772', 'ENSG00000159189', 'ENSG00000160712', 'ENSG00000134853', 'ENSG00000160789', 'ENSG00000168646', 'ENSG00000171720', 'ENSG00000106462', 'ENSG00000179583', 'ENSG00000197614', 'ENSG00000090382', 'ENSG00000134057', 'ENSG00000039068', 'ENSG00000134545', 'ENSG00000168283', 'ENSG00000169245', 'ENSG00000196411', 'ENSG00000104921', 'ENSG00000173269', 'ENSG00000166888', 'ENSG00000130816', 'ENSG00000160791', 'ENSG00000172543', 'ENSG00000115232', 'ENSG00000117560', 'ENSG00000177575', 'ENSG00000133703', 'ENSG00000151882', 'ENSG00000160223', 'ENSG00000108691', 'ENSG00000019991', 'ENSG00000162434', 'ENSG00000123374', 'ENSG00000100219', 'ENSG00000162772', 'ENSG00000078061', 'ENSG00000150630', 'ENSG00000170345', 'ENSG00000113721', 'ENSG00000102245', 'ENSG00000069869', 'ENSG00000100906', 'ENSG00000126561', 'ENSG00000138685', 'ENSG00000114251', 'ENSG00000004799', 'ENSG00000179526', 'ENSG00000118520', 'ENSG00000108821', 'ENSG00000090339', 'ENSG00000148773', 'ENSG00000104365', 'ENSG00000161921', 'ENSG00000174136', 'ENSG00000120156', 'ENSG00000110092', 'ENSG00000081985', 'ENSG00000019549', 'ENSG00000048462', 'ENSG00000105810', 'ENSG00000049249', 'ENSG00000130303', 'ENSG00000110876', 'ENSG00000186810', 'ENSG00000172116', 'ENSG00000185885', 'ENSG00000165168', 'ENSG00000169429', 'ENSG00000076242', 'ENSG00000197646', 'ENSG00000119699', 'ENSG00000116044', 'ENSG00000127528', 'ENSG00000172215', 'ENSG00000105221', 'ENSG00000130635', 'ENSG00000090659', 'ENSG00000138413', 'ENSG00000175054', 'ENSG00000169032', 'ENSG00000160654', 'ENSG00000124216', 'ENSG00000173511', 'ENSG00000135111', 'ENSG00000120708', 'ENSG00000105329', 'ENSG00000161638', 'ENSG00000087245', 'ENSG00000100448', 'ENSG00000121879', 'ENSG00000183765', 'ENSG00000139193', 'ENSG00000106799', 'ENSG00000152952', 'ENSG00000169679', 'ENSG00000095970', 'ENSG00000177494', 'ENSG00000049768', 'ENSG00000198793', 'ENSG00000183813', 'ENSG00000171320', 'ENSG00000113578', 'ENSG00000213949', 'ENSG00000107562', 'ENSG00000037280', 'ENSG00000168487', 'ENSG00000118785', 'ENSG00000158485', 'ENSG00000196611', 'ENSG00000136158', 'ENSG00000169047', 'ENSG00000125538', 'ENSG00000137441', 'ENSG00000105851', 'ENSG00000012124', 'ENSG00000126353', 'ENSG00000101057', 'ENSG00000213281', 'ENSG00000108622', 'ENSG00000175197', 'ENSG00000136244', 'ENSG00000163735', 'ENSG00000133067', 'ENSG00000144476', 'ENSG00000117281', 'ENSG00000159110', 'ENSG00000128602', 'ENSG00000100644', 'ENSG00000141736', 'ENSG00000160255', 'ENSG00000173039', 'ENSG00000156738', 'ENSG00000091879', 'ENSG00000185483', 'ENSG00000107159', 'ENSG00000105369', 'ENSG00000168811', 'ENSG00000170476', 'ENSG00000019169', 'ENSG00000064012', 'ENSG00000115523', 'ENSG00000182580', 'ENSG00000148737', 'ENSG00000155760', 'ENSG00000181847', 'ENSG00000151702', 'ENSG00000178726', 'ENSG00000087088', 'ENSG00000139292', 'ENSG00000067225', 'ENSG00000178562', 'ENSG00000163421', 'ENSG00000121807', 'ENSG00000179776', 'ENSG00000185633', 'ENSG00000167286', 'ENSG00000198574', 'ENSG00000174175', 'ENSG00000143365', 'ENSG00000100453', 'ENSG00000134954', 'ENSG00000145431', 'ENSG00000087586', 'ENSG00000124762', 'ENSG00000106366', 'ENSG00000126262', 'ENSG00000162692', 'ENSG00000186265', 'ENSG00000168610', 'ENSG00000213809', 'ENSG00000136997', 'ENSG00000153563', 'ENSG00000049130', 'ENSG00000171227', 'ENSG00000104951', 'ENSG00000163359', 'ENSG00000030419', 'ENSG00000141510', 'ENSG00000066468', 'ENSG00000184557', 'ENSG00000145649', 'ENSG00000130300', 'ENSG00000137265', 'ENSG00000198178', 'ENSG00000163600', 'ENSG00000182578', 'ENSG00000099250', 'ENSG00000148400', 'ENSG00000077942', 'ENSG00000138798', 'ENSG00000183734', 'ENSG00000109320', 'ENSG00000138795', 'ENSG00000177455', 'ENSG00000196878', 'ENSG00000027697', 'ENSG00000148516', 'ENSG00000112115', 'ENSG00000159958', 'ENSG00000128052', 'ENSG00000142627', 'ENSG00000163519', 'ENSG00000113263', 'ENSG00000088827', 'ENSG00000150637', 'ENSG00000091181', 'ENSG00000110448', 'ENSG00000120738', 'ENSG00000143226', 'ENSG00000102970', 'ENSG00000073861', 'ENSG00000163220', 'ENSG00000073111', 'ENSG00000101412', 'ENSG00000141867', 'ENSG00000116824', 'ENSG00000127616', 'ENSG00000180871', 'ENSG00000030110', 'ENSG00000163823']\n"
     ]
    }
   ],
   "source": [
    "query_genes = [g for g in query_data.var.index if g not in target_genes]\n",
    "query_batches = list(query_data.obs['batch'].unique())\n",
    "ref_batches = list(ref_data.obs['batch'].unique())\n",
    "batch_gene_list = dict(zip(list(query_batches) + list(ref_batches),\n",
    "    [query_genes]*len(query_batches) + [ref_data.var.index.tolist()]*len(ref_batches)))\n",
    "print('query_genes:', query_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96bc99",
   "metadata": {},
   "source": [
    "## Overwrite parts of the default config\n",
    "These hyperparameters are recommended for general purpose. We did not tune it for individual datasets. You may update them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6cccb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'lr': 0.0002,\n",
       "  'wd': 1e-08,\n",
       "  'scheduler': 'plat',\n",
       "  'epochs': 1,\n",
       "  'max_eval_batch_size': 100000,\n",
       "  'patience': 5,\n",
       "  'workers': 0},\n",
       " {'objective': 'SSL',\n",
       "  'mask_node_rate': 0.75,\n",
       "  'mask_feature_rate': 0.25,\n",
       "  'head_type': 'ssl',\n",
       "  'max_batch_size': 70000})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config = SSLDefaultPipelineConfig.copy()\n",
    "model_config = SSLDefaultModelConfig.copy()\n",
    "\n",
    "pipeline_config, model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe80a4",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd48872",
   "metadata": {},
   "source": [
    "Efficient data setup and fine-tuning can be seamlessly conducted using the CellPLM built-in `pipeline` module.\n",
    "\n",
    "First, initialize a `ImputationPipeline`. This pipeline will automatically load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54b753e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in thre vae:::::::::::::::::::::::::::::::::::::::::::::::::::::::::))))))))))))))))))))))))))))))))))))))\n",
      "self.head_type: ssl\n",
      "55555555555555555555555555555555555555555555self.pre_latent_norm = PreLatentNorm(ln, enc_hid)\n",
      "embedder.norm0.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "embedder.norm0.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "embedder.extra_linear.0.weight:\n",
      "Initial weights: tensor([[-0.0219, -0.0008,  0.0304,  ...,  0.0036,  0.0086, -0.0162],\n",
      "        [ 0.0261,  0.0049, -0.0098,  ...,  0.0007, -0.0030,  0.0145],\n",
      "        [ 0.0102, -0.0045, -0.0115,  ...,  0.0306, -0.0108, -0.0021],\n",
      "        ...,\n",
      "        [-0.0011, -0.0138,  0.0118,  ...,  0.0008,  0.0182, -0.0075],\n",
      "        [ 0.0122,  0.0076,  0.0231,  ...,  0.0296, -0.0130, -0.0008],\n",
      "        [ 0.0014, -0.0158,  0.0034,  ..., -0.0281, -0.0267, -0.0173]])\n",
      "embedder.extra_linear.0.bias:\n",
      "Initial weights: tensor([-0.0262,  0.0247, -0.0041,  ...,  0.0117,  0.0010,  0.0041])\n",
      "embedder.extra_linear.3.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "embedder.extra_linear.3.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "embedder.pe_enc.missing_pe:\n",
      "Initial weights: tensor([ 0.0046, -0.0042,  0.0026,  ...,  0.0070, -0.0088, -0.0026])\n",
      "embedder.feat_enc.emb:\n",
      "Initial weights: tensor([[ 0.0040,  0.0022,  0.0055,  ..., -0.0055, -0.0011,  0.0014],\n",
      "        [-0.0043, -0.0047, -0.0005,  ..., -0.0004,  0.0043,  0.0016],\n",
      "        [-0.0029,  0.0007, -0.0071,  ..., -0.0033,  0.0037, -0.0104],\n",
      "        ...,\n",
      "        [-0.0037,  0.0022,  0.0002,  ..., -0.0021,  0.0041, -0.0097],\n",
      "        [ 0.0011,  0.0038,  0.0032,  ...,  0.0088, -0.0014,  0.0038],\n",
      "        [-0.0055, -0.0004,  0.0014,  ...,  0.0023, -0.0009, -0.0013]])\n",
      "encoder.layers.0.self_attn.query_projection.weight:\n",
      "Initial weights: tensor([[-2.0931e-03,  9.1308e-03, -3.0381e-02,  ..., -2.2533e-02,\n",
      "          7.5619e-03,  3.1006e-02],\n",
      "        [ 1.3367e-02, -5.0962e-06,  1.2261e-02,  ..., -1.7328e-02,\n",
      "          6.8689e-03, -1.6552e-02],\n",
      "        [-6.4111e-03, -1.1925e-02,  1.6149e-02,  ...,  2.6254e-02,\n",
      "         -1.9494e-02,  1.9664e-02],\n",
      "        ...,\n",
      "        [ 2.2184e-02,  1.5292e-03,  9.3179e-03,  ..., -1.8880e-02,\n",
      "          2.7289e-02, -1.7531e-02],\n",
      "        [-2.4493e-02, -1.1983e-02,  1.4344e-02,  ...,  4.8476e-03,\n",
      "         -2.2156e-02, -2.3434e-02],\n",
      "        [ 2.0138e-02, -1.7490e-02,  1.8662e-02,  ...,  2.1384e-02,\n",
      "         -4.8047e-03, -1.3057e-02]])\n",
      "encoder.layers.0.self_attn.query_projection.bias:\n",
      "Initial weights: tensor([ 0.0026, -0.0281,  0.0278,  ..., -0.0093,  0.0226,  0.0083])\n",
      "encoder.layers.0.self_attn.key_projection.weight:\n",
      "Initial weights: tensor([[-0.0259, -0.0196,  0.0295,  ...,  0.0040, -0.0150, -0.0229],\n",
      "        [ 0.0145,  0.0178, -0.0249,  ..., -0.0197, -0.0015,  0.0127],\n",
      "        [ 0.0226,  0.0092, -0.0176,  ..., -0.0024,  0.0085,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0040, -0.0081, -0.0172,  ...,  0.0291,  0.0143, -0.0146],\n",
      "        [ 0.0288, -0.0286, -0.0005,  ..., -0.0148, -0.0110,  0.0162],\n",
      "        [-0.0245, -0.0125,  0.0241,  ...,  0.0238, -0.0013, -0.0169]])\n",
      "encoder.layers.0.self_attn.key_projection.bias:\n",
      "Initial weights: tensor([ 0.0154,  0.0112, -0.0138,  ...,  0.0246, -0.0004, -0.0167])\n",
      "encoder.layers.0.self_attn.value_projection.weight:\n",
      "Initial weights: tensor([[ 0.0081,  0.0197, -0.0059,  ...,  0.0273, -0.0158,  0.0206],\n",
      "        [-0.0055,  0.0223, -0.0107,  ..., -0.0186,  0.0157,  0.0289],\n",
      "        [ 0.0160,  0.0110,  0.0054,  ...,  0.0116,  0.0144, -0.0258],\n",
      "        ...,\n",
      "        [ 0.0177,  0.0276, -0.0268,  ...,  0.0216, -0.0182, -0.0254],\n",
      "        [ 0.0022, -0.0228, -0.0296,  ...,  0.0020,  0.0059,  0.0097],\n",
      "        [ 0.0244,  0.0168, -0.0094,  ..., -0.0057, -0.0186,  0.0287]])\n",
      "encoder.layers.0.self_attn.value_projection.bias:\n",
      "Initial weights: tensor([-0.0234,  0.0141, -0.0079,  ..., -0.0286, -0.0221, -0.0127])\n",
      "encoder.layers.0.self_attn.out_projection.weight:\n",
      "Initial weights: tensor([[-0.0042, -0.0164, -0.0077,  ..., -0.0050,  0.0280, -0.0205],\n",
      "        [ 0.0108, -0.0062, -0.0169,  ..., -0.0192,  0.0141,  0.0080],\n",
      "        [-0.0078,  0.0040,  0.0169,  ...,  0.0168, -0.0187,  0.0232],\n",
      "        ...,\n",
      "        [-0.0175, -0.0070,  0.0199,  ...,  0.0283,  0.0194, -0.0086],\n",
      "        [-0.0016,  0.0147,  0.0182,  ..., -0.0240, -0.0218, -0.0094],\n",
      "        [-0.0041, -0.0203,  0.0237,  ...,  0.0212,  0.0090, -0.0166]])\n",
      "encoder.layers.0.self_attn.out_projection.bias:\n",
      "Initial weights: tensor([-0.0207, -0.0223, -0.0005,  ...,  0.0248, -0.0053, -0.0133])\n",
      "encoder.layers.0._ff_block.0.weight:\n",
      "Initial weights: tensor([[ 0.0135,  0.0091,  0.0267,  ...,  0.0013,  0.0273,  0.0117],\n",
      "        [-0.0252,  0.0244,  0.0015,  ...,  0.0273, -0.0127,  0.0098],\n",
      "        [ 0.0259,  0.0053, -0.0150,  ...,  0.0250, -0.0147, -0.0240],\n",
      "        ...,\n",
      "        [ 0.0096,  0.0263,  0.0067,  ..., -0.0279, -0.0098, -0.0255],\n",
      "        [ 0.0038, -0.0002, -0.0284,  ..., -0.0071, -0.0092,  0.0075],\n",
      "        [ 0.0101,  0.0286,  0.0168,  ..., -0.0082,  0.0030, -0.0212]])\n",
      "encoder.layers.0._ff_block.0.bias:\n",
      "Initial weights: tensor([-0.0081,  0.0009, -0.0168,  ...,  0.0166,  0.0273, -0.0266])\n",
      "encoder.layers.0._ff_block.3.weight:\n",
      "Initial weights: tensor([[ 0.0023, -0.0169,  0.0077,  ...,  0.0180, -0.0160, -0.0208],\n",
      "        [ 0.0042,  0.0121,  0.0046,  ...,  0.0063,  0.0011, -0.0096],\n",
      "        [ 0.0101, -0.0091,  0.0101,  ...,  0.0033,  0.0217, -0.0133],\n",
      "        ...,\n",
      "        [ 0.0077, -0.0062,  0.0091,  ..., -0.0161,  0.0048,  0.0018],\n",
      "        [ 0.0173, -0.0110, -0.0109,  ...,  0.0142,  0.0042,  0.0035],\n",
      "        [-0.0069,  0.0144,  0.0066,  ..., -0.0048,  0.0045,  0.0093]])\n",
      "encoder.layers.0._ff_block.3.bias:\n",
      "Initial weights: tensor([-0.0025, -0.0009,  0.0219,  ..., -0.0151,  0.0078,  0.0114])\n",
      "encoder.layers.0.norm1.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.0.norm1.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.0.norm2.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.0.norm2.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.1.self_attn.query_projection.weight:\n",
      "Initial weights: tensor([[ 0.0075,  0.0198, -0.0284,  ..., -0.0053,  0.0229,  0.0063],\n",
      "        [-0.0198, -0.0276, -0.0235,  ..., -0.0028, -0.0076, -0.0031],\n",
      "        [ 0.0259,  0.0311, -0.0130,  ..., -0.0143, -0.0108, -0.0225],\n",
      "        ...,\n",
      "        [-0.0221, -0.0157, -0.0009,  ...,  0.0302,  0.0133,  0.0050],\n",
      "        [ 0.0104, -0.0177,  0.0046,  ...,  0.0277,  0.0022, -0.0041],\n",
      "        [ 0.0148,  0.0141,  0.0017,  ..., -0.0279,  0.0065, -0.0205]])\n",
      "encoder.layers.1.self_attn.query_projection.bias:\n",
      "Initial weights: tensor([ 0.0295, -0.0136, -0.0246,  ...,  0.0176, -0.0107, -0.0180])\n",
      "encoder.layers.1.self_attn.key_projection.weight:\n",
      "Initial weights: tensor([[-0.0287,  0.0104, -0.0260,  ..., -0.0143,  0.0049,  0.0040],\n",
      "        [ 0.0063, -0.0151,  0.0009,  ...,  0.0142,  0.0254, -0.0211],\n",
      "        [ 0.0066, -0.0300,  0.0096,  ...,  0.0059,  0.0073, -0.0281],\n",
      "        ...,\n",
      "        [-0.0073, -0.0268, -0.0278,  ..., -0.0296, -0.0133, -0.0126],\n",
      "        [-0.0043,  0.0025,  0.0259,  ...,  0.0070,  0.0210,  0.0181],\n",
      "        [-0.0191,  0.0168, -0.0293,  ...,  0.0212,  0.0061,  0.0193]])\n",
      "encoder.layers.1.self_attn.key_projection.bias:\n",
      "Initial weights: tensor([ 0.0157,  0.0123, -0.0148,  ..., -0.0263,  0.0027, -0.0204])\n",
      "encoder.layers.1.self_attn.value_projection.weight:\n",
      "Initial weights: tensor([[ 0.0154,  0.0241,  0.0173,  ..., -0.0213, -0.0031,  0.0272],\n",
      "        [ 0.0091, -0.0172, -0.0152,  ..., -0.0143, -0.0048, -0.0196],\n",
      "        [-0.0084,  0.0270,  0.0311,  ...,  0.0207, -0.0116,  0.0207],\n",
      "        ...,\n",
      "        [ 0.0169,  0.0228,  0.0015,  ..., -0.0269,  0.0097, -0.0164],\n",
      "        [-0.0181, -0.0308,  0.0138,  ..., -0.0178, -0.0053,  0.0025],\n",
      "        [ 0.0140, -0.0221,  0.0279,  ..., -0.0235,  0.0025,  0.0115]])\n",
      "encoder.layers.1.self_attn.value_projection.bias:\n",
      "Initial weights: tensor([-0.0215,  0.0063, -0.0077,  ...,  0.0189,  0.0096,  0.0153])\n",
      "encoder.layers.1.self_attn.out_projection.weight:\n",
      "Initial weights: tensor([[-0.0089,  0.0099, -0.0088,  ...,  0.0186, -0.0189, -0.0264],\n",
      "        [ 0.0024, -0.0286, -0.0113,  ..., -0.0111,  0.0167, -0.0152],\n",
      "        [ 0.0243,  0.0082,  0.0205,  ...,  0.0228, -0.0130, -0.0150],\n",
      "        ...,\n",
      "        [-0.0086,  0.0008, -0.0121,  ...,  0.0250,  0.0008,  0.0148],\n",
      "        [ 0.0207,  0.0297, -0.0194,  ...,  0.0238,  0.0176,  0.0200],\n",
      "        [-0.0082,  0.0077, -0.0219,  ..., -0.0007,  0.0119, -0.0002]])\n",
      "encoder.layers.1.self_attn.out_projection.bias:\n",
      "Initial weights: tensor([ 0.0050,  0.0142, -0.0005,  ..., -0.0128, -0.0201,  0.0027])\n",
      "encoder.layers.1._ff_block.0.weight:\n",
      "Initial weights: tensor([[ 0.0069, -0.0228,  0.0080,  ..., -0.0057, -0.0104,  0.0009],\n",
      "        [-0.0087, -0.0079,  0.0143,  ...,  0.0203,  0.0266, -0.0066],\n",
      "        [-0.0210,  0.0147, -0.0109,  ...,  0.0240, -0.0096, -0.0219],\n",
      "        ...,\n",
      "        [-0.0285, -0.0082,  0.0272,  ...,  0.0168,  0.0143, -0.0129],\n",
      "        [-0.0202,  0.0286,  0.0062,  ...,  0.0208,  0.0207, -0.0183],\n",
      "        [ 0.0161, -0.0217, -0.0209,  ..., -0.0082,  0.0204,  0.0054]])\n",
      "encoder.layers.1._ff_block.0.bias:\n",
      "Initial weights: tensor([-0.0274,  0.0074, -0.0072,  ..., -0.0279, -0.0173,  0.0266])\n",
      "encoder.layers.1._ff_block.3.weight:\n",
      "Initial weights: tensor([[ 0.0201, -0.0050,  0.0094,  ...,  0.0167, -0.0100,  0.0192],\n",
      "        [ 0.0182,  0.0174,  0.0202,  ..., -0.0051,  0.0200, -0.0035],\n",
      "        [ 0.0087, -0.0196,  0.0100,  ..., -0.0009,  0.0032, -0.0208],\n",
      "        ...,\n",
      "        [ 0.0039, -0.0141, -0.0096,  ..., -0.0158,  0.0085,  0.0055],\n",
      "        [ 0.0109, -0.0180, -0.0205,  ..., -0.0079, -0.0073, -0.0109],\n",
      "        [ 0.0190,  0.0106, -0.0084,  ...,  0.0089,  0.0206, -0.0106]])\n",
      "encoder.layers.1._ff_block.3.bias:\n",
      "Initial weights: tensor([ 0.0157,  0.0175, -0.0131,  ..., -0.0095, -0.0191, -0.0145])\n",
      "encoder.layers.1.norm1.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.1.norm1.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.1.norm2.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.1.norm2.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.2.self_attn.query_projection.weight:\n",
      "Initial weights: tensor([[-1.4734e-03, -1.8075e-02,  5.1003e-03,  ...,  2.0732e-02,\n",
      "          2.9700e-02, -2.9632e-02],\n",
      "        [ 8.2831e-03, -6.0076e-03, -2.9064e-02,  ...,  1.7701e-02,\n",
      "         -1.6062e-02,  2.7008e-02],\n",
      "        [-2.3011e-02, -8.1136e-03,  3.1583e-04,  ...,  2.7625e-02,\n",
      "         -1.1056e-02, -2.2287e-02],\n",
      "        ...,\n",
      "        [-1.1194e-02,  3.0597e-02, -2.2756e-02,  ...,  9.1113e-03,\n",
      "          3.0849e-02, -2.0311e-02],\n",
      "        [-1.9339e-02, -2.8400e-02, -2.3494e-02,  ...,  1.9916e-03,\n",
      "          6.8538e-05, -2.6462e-02],\n",
      "        [ 1.4910e-02, -3.3218e-03,  1.5287e-02,  ...,  3.4730e-03,\n",
      "          1.3709e-02,  1.2112e-02]])\n",
      "encoder.layers.2.self_attn.query_projection.bias:\n",
      "Initial weights: tensor([ 0.0161,  0.0237, -0.0112,  ...,  0.0128,  0.0247,  0.0300])\n",
      "encoder.layers.2.self_attn.key_projection.weight:\n",
      "Initial weights: tensor([[-0.0129, -0.0106, -0.0123,  ..., -0.0225, -0.0081, -0.0245],\n",
      "        [-0.0244, -0.0136, -0.0178,  ..., -0.0292, -0.0230,  0.0170],\n",
      "        [ 0.0218, -0.0213,  0.0034,  ...,  0.0276, -0.0287, -0.0272],\n",
      "        ...,\n",
      "        [ 0.0189, -0.0104, -0.0126,  ..., -0.0263, -0.0058, -0.0253],\n",
      "        [-0.0232, -0.0135, -0.0300,  ..., -0.0216, -0.0010, -0.0146],\n",
      "        [-0.0278,  0.0186,  0.0140,  ...,  0.0167, -0.0112,  0.0204]])\n",
      "encoder.layers.2.self_attn.key_projection.bias:\n",
      "Initial weights: tensor([ 0.0288,  0.0137, -0.0137,  ..., -0.0178,  0.0153, -0.0283])\n",
      "encoder.layers.2.self_attn.value_projection.weight:\n",
      "Initial weights: tensor([[ 1.8874e-02, -2.8644e-02, -2.6178e-02,  ...,  3.0787e-02,\n",
      "         -1.1582e-02, -1.5544e-02],\n",
      "        [-2.7115e-02, -2.8852e-02,  2.3650e-02,  ...,  1.2087e-02,\n",
      "          5.0941e-03, -2.3366e-03],\n",
      "        [-8.9671e-03, -2.8423e-02, -2.9415e-02,  ..., -1.1675e-02,\n",
      "          4.3441e-03,  2.9537e-02],\n",
      "        ...,\n",
      "        [-2.1325e-02, -1.2926e-02, -2.9735e-03,  ..., -2.9921e-02,\n",
      "         -2.8038e-02, -2.7278e-02],\n",
      "        [ 8.6477e-03, -2.4013e-02, -2.5821e-02,  ...,  2.6789e-02,\n",
      "          8.9802e-04, -4.1246e-03],\n",
      "        [-7.2189e-05,  2.5353e-03, -1.7564e-03,  ...,  2.3989e-02,\n",
      "         -1.9007e-02,  2.8335e-02]])\n",
      "encoder.layers.2.self_attn.value_projection.bias:\n",
      "Initial weights: tensor([-0.0009,  0.0142, -0.0202,  ..., -0.0250,  0.0186,  0.0183])\n",
      "encoder.layers.2.self_attn.out_projection.weight:\n",
      "Initial weights: tensor([[-0.0024,  0.0035,  0.0264,  ..., -0.0220, -0.0144, -0.0249],\n",
      "        [-0.0037, -0.0219, -0.0023,  ...,  0.0306, -0.0173, -0.0193],\n",
      "        [ 0.0165,  0.0187, -0.0184,  ...,  0.0241, -0.0041, -0.0292],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0293,  0.0086,  ...,  0.0176, -0.0115, -0.0053],\n",
      "        [-0.0124, -0.0015,  0.0158,  ...,  0.0142,  0.0165, -0.0105],\n",
      "        [-0.0056,  0.0129,  0.0230,  ...,  0.0091, -0.0095,  0.0131]])\n",
      "encoder.layers.2.self_attn.out_projection.bias:\n",
      "Initial weights: tensor([-0.0065,  0.0290, -0.0228,  ...,  0.0277, -0.0277,  0.0254])\n",
      "encoder.layers.2._ff_block.0.weight:\n",
      "Initial weights: tensor([[-0.0235,  0.0035,  0.0272,  ..., -0.0193, -0.0008, -0.0137],\n",
      "        [-0.0166, -0.0038, -0.0173,  ...,  0.0104, -0.0090,  0.0121],\n",
      "        [ 0.0029, -0.0263, -0.0132,  ..., -0.0229,  0.0198,  0.0214],\n",
      "        ...,\n",
      "        [-0.0252,  0.0088, -0.0193,  ..., -0.0026,  0.0165, -0.0209],\n",
      "        [ 0.0254,  0.0189, -0.0083,  ...,  0.0193,  0.0027,  0.0203],\n",
      "        [ 0.0269, -0.0169, -0.0100,  ...,  0.0262,  0.0096, -0.0094]])\n",
      "encoder.layers.2._ff_block.0.bias:\n",
      "Initial weights: tensor([-0.0115, -0.0002, -0.0051,  ...,  0.0050,  0.0242,  0.0303])\n",
      "encoder.layers.2._ff_block.3.weight:\n",
      "Initial weights: tensor([[-0.0166,  0.0011,  0.0033,  ...,  0.0138,  0.0193,  0.0022],\n",
      "        [-0.0127, -0.0086,  0.0016,  ...,  0.0211, -0.0125,  0.0172],\n",
      "        [ 0.0154, -0.0194, -0.0199,  ...,  0.0055,  0.0084,  0.0179],\n",
      "        ...,\n",
      "        [-0.0214, -0.0148, -0.0016,  ..., -0.0076,  0.0192, -0.0062],\n",
      "        [ 0.0016, -0.0180, -0.0129,  ...,  0.0169,  0.0209, -0.0137],\n",
      "        [ 0.0163,  0.0079,  0.0034,  ...,  0.0075,  0.0129, -0.0145]])\n",
      "encoder.layers.2._ff_block.3.bias:\n",
      "Initial weights: tensor([-0.0031, -0.0070, -0.0194,  ..., -0.0135, -0.0179, -0.0118])\n",
      "encoder.layers.2.norm1.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.2.norm1.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.2.norm2.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.2.norm2.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.3.self_attn.query_projection.weight:\n",
      "Initial weights: tensor([[-0.0005,  0.0071, -0.0063,  ...,  0.0224, -0.0306, -0.0067],\n",
      "        [ 0.0163, -0.0046, -0.0096,  ..., -0.0205, -0.0284, -0.0054],\n",
      "        [-0.0146,  0.0212, -0.0111,  ...,  0.0202,  0.0223,  0.0241],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0104,  0.0205,  ..., -0.0268,  0.0207,  0.0091],\n",
      "        [-0.0251,  0.0122, -0.0154,  ..., -0.0141,  0.0100, -0.0255],\n",
      "        [-0.0054,  0.0200, -0.0177,  ...,  0.0034,  0.0170,  0.0208]])\n",
      "encoder.layers.3.self_attn.query_projection.bias:\n",
      "Initial weights: tensor([ 0.0079,  0.0051,  0.0279,  ..., -0.0072, -0.0270,  0.0160])\n",
      "encoder.layers.3.self_attn.key_projection.weight:\n",
      "Initial weights: tensor([[-0.0030, -0.0031, -0.0078,  ...,  0.0036, -0.0183, -0.0148],\n",
      "        [ 0.0190,  0.0039,  0.0024,  ..., -0.0190, -0.0098, -0.0038],\n",
      "        [-0.0074,  0.0024,  0.0252,  ...,  0.0279,  0.0117, -0.0090],\n",
      "        ...,\n",
      "        [-0.0247,  0.0158,  0.0023,  ...,  0.0014,  0.0247,  0.0195],\n",
      "        [-0.0203,  0.0088,  0.0073,  ..., -0.0248, -0.0017,  0.0232],\n",
      "        [-0.0133, -0.0134, -0.0233,  ...,  0.0240, -0.0274,  0.0005]])\n",
      "encoder.layers.3.self_attn.key_projection.bias:\n",
      "Initial weights: tensor([-0.0133,  0.0299, -0.0168,  ...,  0.0220,  0.0086,  0.0304])\n",
      "encoder.layers.3.self_attn.value_projection.weight:\n",
      "Initial weights: tensor([[-0.0060,  0.0134,  0.0280,  ..., -0.0062, -0.0103,  0.0110],\n",
      "        [-0.0250,  0.0202, -0.0087,  ...,  0.0230,  0.0275, -0.0148],\n",
      "        [-0.0010, -0.0277,  0.0092,  ...,  0.0059, -0.0222,  0.0113],\n",
      "        ...,\n",
      "        [ 0.0058, -0.0310, -0.0064,  ..., -0.0193, -0.0022, -0.0076],\n",
      "        [-0.0264, -0.0118, -0.0270,  ...,  0.0128, -0.0117, -0.0149],\n",
      "        [ 0.0010,  0.0099,  0.0137,  ...,  0.0195, -0.0307, -0.0144]])\n",
      "encoder.layers.3.self_attn.value_projection.bias:\n",
      "Initial weights: tensor([ 0.0040, -0.0240, -0.0221,  ..., -0.0185,  0.0215,  0.0110])\n",
      "encoder.layers.3.self_attn.out_projection.weight:\n",
      "Initial weights: tensor([[-0.0047, -0.0233,  0.0167,  ...,  0.0146, -0.0172,  0.0138],\n",
      "        [-0.0038,  0.0094, -0.0071,  ...,  0.0224,  0.0132,  0.0151],\n",
      "        [-0.0090, -0.0164,  0.0290,  ...,  0.0043,  0.0171, -0.0197],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0274,  0.0078,  ..., -0.0146,  0.0311,  0.0052],\n",
      "        [-0.0202, -0.0180, -0.0259,  ...,  0.0244,  0.0052,  0.0190],\n",
      "        [ 0.0168, -0.0109, -0.0192,  ..., -0.0014, -0.0027, -0.0093]])\n",
      "encoder.layers.3.self_attn.out_projection.bias:\n",
      "Initial weights: tensor([ 0.0064, -0.0031, -0.0003,  ..., -0.0116, -0.0037,  0.0245])\n",
      "encoder.layers.3._ff_block.0.weight:\n",
      "Initial weights: tensor([[-0.0285,  0.0206, -0.0011,  ...,  0.0253, -0.0264,  0.0240],\n",
      "        [ 0.0109,  0.0066, -0.0050,  ..., -0.0062,  0.0119,  0.0067],\n",
      "        [-0.0073,  0.0219, -0.0182,  ...,  0.0247, -0.0028, -0.0067],\n",
      "        ...,\n",
      "        [-0.0080,  0.0180,  0.0002,  ..., -0.0252,  0.0094, -0.0125],\n",
      "        [ 0.0293,  0.0078, -0.0002,  ...,  0.0037,  0.0224, -0.0111],\n",
      "        [ 0.0112, -0.0146, -0.0109,  ...,  0.0204,  0.0132, -0.0098]])\n",
      "encoder.layers.3._ff_block.0.bias:\n",
      "Initial weights: tensor([ 0.0288,  0.0007, -0.0231,  ...,  0.0229, -0.0046,  0.0161])\n",
      "encoder.layers.3._ff_block.3.weight:\n",
      "Initial weights: tensor([[ 1.8540e-02, -1.2805e-02, -5.2055e-03,  ...,  7.3667e-03,\n",
      "         -9.1113e-03, -1.0253e-03],\n",
      "        [ 2.5152e-03, -2.0563e-02,  1.1083e-02,  ...,  8.0088e-03,\n",
      "         -7.6492e-03,  7.5715e-03],\n",
      "        [ 6.3057e-03, -1.3329e-03, -4.9785e-03,  ..., -1.0412e-02,\n",
      "         -2.0560e-02, -1.8832e-02],\n",
      "        ...,\n",
      "        [ 1.9432e-02, -1.2473e-02,  2.1617e-02,  ...,  1.9169e-02,\n",
      "         -6.4003e-05, -1.1868e-02],\n",
      "        [-1.2651e-02,  1.5356e-02, -7.1067e-04,  ...,  2.0569e-03,\n",
      "         -1.8778e-02, -1.8168e-03],\n",
      "        [-1.8660e-02,  3.2848e-03, -7.0775e-03,  ...,  1.5091e-02,\n",
      "         -9.9592e-04, -1.0634e-02]])\n",
      "encoder.layers.3._ff_block.3.bias:\n",
      "Initial weights: tensor([-0.0102, -0.0029,  0.0200,  ..., -0.0036, -0.0159, -0.0136])\n",
      "encoder.layers.3.norm1.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.3.norm1.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "encoder.layers.3.norm2.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "encoder.layers.3.norm2.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "latent.layers.1.hid_2mu.weight:\n",
      "Initial weights: tensor([[ 0.0263, -0.0198,  0.0079,  ..., -0.0158,  0.0306,  0.0125],\n",
      "        [-0.0148,  0.0231,  0.0215,  ...,  0.0068, -0.0282, -0.0288],\n",
      "        [ 0.0267,  0.0116,  0.0290,  ..., -0.0183, -0.0247, -0.0112],\n",
      "        ...,\n",
      "        [-0.0204, -0.0178, -0.0153,  ..., -0.0276,  0.0278,  0.0033],\n",
      "        [-0.0267, -0.0251,  0.0195,  ..., -0.0105,  0.0098, -0.0259],\n",
      "        [-0.0214,  0.0029,  0.0071,  ...,  0.0081, -0.0009, -0.0076]])\n",
      "latent.layers.1.hid_2mu.bias:\n",
      "Initial weights: tensor([-2.6657e-02,  6.5061e-03,  2.5276e-02, -2.4344e-02, -8.4048e-03,\n",
      "        -2.1781e-02,  4.3447e-03, -1.8280e-02,  2.3200e-02,  2.5381e-02,\n",
      "         2.3320e-02,  1.9446e-02, -1.4858e-02,  1.3676e-02,  1.5794e-02,\n",
      "        -5.3947e-03, -1.6655e-02, -2.9743e-02, -1.2575e-02,  6.1406e-03,\n",
      "         9.7135e-03,  2.0302e-02,  2.2201e-02, -9.7022e-03,  2.2142e-02,\n",
      "         2.0120e-02,  1.8041e-02,  5.7652e-03,  2.5999e-02, -3.1216e-03,\n",
      "        -5.2758e-03,  2.7819e-02, -1.7928e-02,  2.1215e-03, -4.9126e-03,\n",
      "        -3.0971e-02,  2.3988e-02, -1.9123e-02,  1.1556e-02,  5.0217e-03,\n",
      "        -2.4908e-02,  2.5797e-03, -1.8309e-02,  2.2290e-02, -3.1175e-02,\n",
      "         3.1037e-02, -1.3024e-02,  2.5547e-02, -2.7561e-03,  2.4575e-02,\n",
      "        -2.2082e-02,  2.6595e-02,  9.5138e-03, -2.5110e-02, -5.2837e-03,\n",
      "         3.0519e-02, -1.1696e-02, -2.2770e-02,  2.5165e-02,  1.5594e-02,\n",
      "        -7.1245e-03, -1.6491e-02, -9.5643e-03,  2.8086e-02, -1.5677e-02,\n",
      "         1.1016e-02, -1.6981e-02, -2.4950e-02, -1.1564e-02, -5.6769e-03,\n",
      "         3.6564e-03, -2.9993e-02,  3.6466e-03, -9.9836e-03, -2.0828e-02,\n",
      "         7.2396e-03,  2.0738e-02,  2.1143e-02, -1.8941e-02, -2.8888e-02,\n",
      "        -9.0438e-03,  6.6736e-03,  2.3810e-02, -4.3163e-04,  2.4659e-02,\n",
      "         2.6235e-02,  1.5988e-02,  4.8947e-03, -2.7889e-02, -1.7177e-03,\n",
      "         2.1636e-02, -1.1837e-02, -2.4729e-02,  2.1655e-02,  8.5895e-03,\n",
      "        -1.8587e-03,  1.1946e-02,  1.3989e-02, -2.9817e-02,  1.5560e-02,\n",
      "         2.9372e-02, -1.2615e-02, -1.8821e-02, -8.2895e-03, -1.9543e-02,\n",
      "         2.5277e-02, -1.2441e-02,  1.5923e-02, -2.6966e-02, -5.8162e-03,\n",
      "        -1.4053e-02, -1.1367e-03, -1.1479e-02,  1.1679e-02,  9.0175e-03,\n",
      "        -2.8823e-02,  2.5981e-02, -2.0992e-02,  1.6600e-02,  1.1143e-02,\n",
      "        -8.5980e-03, -1.3062e-02,  1.5590e-02, -1.2535e-02,  2.8368e-02,\n",
      "         2.4540e-02,  5.4794e-03,  1.2109e-02, -1.6130e-04,  5.3677e-04,\n",
      "         2.5542e-02,  1.6581e-03, -2.5537e-02, -2.6191e-02,  9.3121e-03,\n",
      "         1.8988e-02,  3.1046e-02, -2.6582e-02, -2.6657e-02,  2.6720e-02,\n",
      "        -1.2892e-02,  2.1211e-02,  2.8735e-02, -2.7021e-02,  2.2483e-02,\n",
      "         2.5068e-02,  7.5111e-03, -2.1237e-02,  2.8688e-02, -2.2414e-02,\n",
      "         1.7669e-02, -2.2321e-02,  2.6836e-02, -1.4600e-02,  9.0135e-03,\n",
      "         2.3950e-05,  2.4732e-03,  2.1150e-02, -3.1120e-03, -2.6149e-02,\n",
      "         2.0709e-02, -2.3303e-02, -4.9281e-03,  2.7772e-03, -9.3930e-03,\n",
      "         1.7314e-02, -2.3778e-02,  7.3154e-03, -2.1660e-02,  2.2568e-02,\n",
      "         2.4334e-02,  2.4226e-03,  2.4166e-02,  2.2141e-02,  9.6293e-03,\n",
      "         5.9516e-03, -1.3787e-02,  1.2502e-02, -8.8547e-03,  2.0583e-02,\n",
      "        -2.0761e-03, -1.1353e-02, -7.4426e-04, -2.5895e-02,  1.8558e-03,\n",
      "         1.8622e-02,  7.1005e-03, -2.2543e-02, -1.0981e-03,  2.9359e-02,\n",
      "         1.4014e-02, -2.7519e-02, -1.2181e-02,  1.9642e-02, -1.8703e-02,\n",
      "        -2.2179e-02, -2.5350e-02, -2.8057e-02,  2.8203e-02,  1.5656e-02,\n",
      "         1.4338e-02, -8.7359e-03,  3.1259e-03, -2.4634e-02,  2.7272e-03,\n",
      "         1.3165e-02, -7.9987e-03,  4.9117e-03, -1.7297e-02,  4.4481e-03,\n",
      "         9.1211e-03, -1.5451e-02, -3.0463e-02, -1.9226e-02,  1.0154e-02,\n",
      "        -5.8072e-03, -1.1160e-02,  1.0876e-02, -2.2666e-02, -2.7881e-02,\n",
      "        -9.8253e-03, -2.2376e-02,  6.2643e-03, -2.1435e-02,  8.0145e-04,\n",
      "        -1.3568e-02,  2.0531e-03,  1.8738e-02,  5.8706e-03,  1.2164e-02,\n",
      "         6.1824e-03, -1.3138e-02, -2.0768e-02,  2.8418e-02, -1.6706e-02,\n",
      "        -2.0161e-02, -1.2200e-02,  1.4739e-02,  1.5788e-02,  1.6392e-02,\n",
      "         1.2505e-02,  1.1167e-02,  1.2585e-03, -1.2076e-02,  8.0356e-03,\n",
      "        -2.3346e-02, -2.6157e-02,  5.6050e-03, -4.1591e-03,  2.9180e-03,\n",
      "         2.9303e-02, -1.7421e-03, -2.0974e-02, -1.5760e-02,  1.0609e-02,\n",
      "        -6.0018e-03, -8.3283e-03, -8.0264e-03, -1.0415e-03,  2.4919e-02,\n",
      "        -1.4746e-02, -2.2845e-02, -2.1293e-02,  2.7046e-02,  2.2713e-02,\n",
      "         2.8779e-02, -1.4048e-02,  2.7872e-03, -2.7754e-02,  1.8068e-02,\n",
      "        -1.2773e-02,  1.8036e-02,  1.1815e-02,  1.2046e-02, -2.2411e-03,\n",
      "         1.2263e-02, -6.1685e-03,  5.3118e-03,  9.4505e-03, -3.6278e-03,\n",
      "        -2.3334e-02,  1.1500e-02,  2.6552e-02,  2.6040e-02, -1.9320e-03,\n",
      "        -3.9974e-03, -2.9182e-02,  2.2099e-03,  1.5358e-04, -9.7143e-03,\n",
      "         1.6719e-02,  2.5087e-02, -2.7027e-02,  2.4288e-03,  1.4893e-02,\n",
      "         2.8561e-02, -7.2812e-03, -2.2470e-02,  2.7745e-02, -1.9837e-02,\n",
      "         1.5435e-02, -1.9137e-02,  2.2948e-02,  2.2338e-02,  3.1048e-02,\n",
      "         9.5740e-03, -2.0339e-03, -2.2678e-02,  1.7210e-02,  1.3294e-02,\n",
      "        -1.3276e-02,  2.5089e-03,  2.4310e-02,  9.6462e-03, -1.8775e-02,\n",
      "        -2.5475e-02,  1.8884e-02, -3.0727e-02,  2.0163e-02, -1.0038e-02,\n",
      "         2.5391e-02,  1.8094e-02,  2.4546e-02, -1.3521e-02,  3.1006e-02,\n",
      "         2.8683e-02, -5.1897e-03, -2.3584e-02, -1.1197e-02,  4.6779e-03,\n",
      "        -9.1622e-03,  1.5143e-02,  2.2918e-02, -2.5395e-02,  2.8057e-02,\n",
      "         4.4585e-03, -2.3972e-02,  2.6146e-02,  2.4299e-02,  1.9805e-02,\n",
      "         2.3459e-02,  3.1039e-02,  1.2703e-02, -6.0403e-03, -6.0621e-03,\n",
      "        -1.1498e-02,  1.5178e-02, -1.3787e-02, -2.5710e-02, -1.8648e-02,\n",
      "        -9.0235e-03, -2.1332e-02,  2.4391e-02,  2.8569e-02, -1.9430e-02,\n",
      "         1.4960e-02,  4.9176e-03, -2.0943e-02, -1.8093e-02,  1.9180e-02,\n",
      "        -1.4955e-02, -1.8354e-02, -8.8216e-03, -1.2408e-02, -7.8397e-03,\n",
      "        -2.6910e-02,  3.0468e-02, -1.4254e-02,  1.4015e-02,  5.3798e-03,\n",
      "         7.3750e-03,  4.3690e-03, -2.3709e-02,  1.5077e-02,  1.6563e-02,\n",
      "        -1.2829e-02,  5.7870e-03, -2.2661e-02,  1.9147e-02,  2.1761e-02,\n",
      "         2.1254e-04, -2.2859e-02,  2.0064e-02, -2.2401e-02,  1.0238e-02,\n",
      "         5.9562e-03, -4.9944e-03, -1.6212e-02, -5.0929e-03, -3.0838e-02,\n",
      "        -3.0327e-02, -1.3203e-02,  1.9037e-02,  9.9092e-03, -2.7815e-03,\n",
      "        -1.4444e-02,  1.3033e-02, -2.8527e-02, -2.1291e-02, -2.6100e-02,\n",
      "        -1.7082e-02,  2.6824e-02,  4.2835e-03,  2.8005e-02, -1.4735e-02,\n",
      "        -3.0504e-02, -4.1776e-03, -3.0811e-02,  1.5429e-02,  1.5678e-02,\n",
      "         1.3180e-04,  1.9073e-02,  3.0125e-02, -2.5201e-02, -2.4858e-03,\n",
      "         2.6574e-02, -3.0929e-02,  1.1108e-02, -6.9538e-03,  3.3709e-03,\n",
      "         3.0473e-02,  2.5815e-02, -1.8123e-02,  2.4663e-02,  6.2654e-03,\n",
      "        -2.5770e-02,  2.8420e-02,  2.8380e-02, -3.0935e-02,  2.6535e-02,\n",
      "        -2.3161e-02, -9.0507e-03,  1.6477e-02, -6.0640e-03, -2.1234e-02,\n",
      "         1.5371e-02, -1.7777e-02, -2.2057e-02,  2.1764e-02, -1.9836e-02,\n",
      "         2.5135e-02, -1.4695e-02, -2.7987e-02, -8.1713e-03,  2.6222e-02,\n",
      "         2.7091e-02, -2.5541e-02,  1.8186e-02, -3.0035e-03,  1.5400e-03,\n",
      "         2.6421e-02, -1.2428e-02, -5.3956e-03, -2.6652e-02,  2.0749e-02,\n",
      "         2.8152e-03, -2.0129e-02,  8.0516e-04,  1.6701e-02,  5.1396e-03,\n",
      "        -2.2521e-02, -6.0583e-04, -3.2586e-03,  2.9604e-02, -2.8210e-02,\n",
      "        -3.4245e-03, -8.2733e-04,  5.7428e-03, -2.7619e-03, -1.4330e-02,\n",
      "        -2.7107e-02,  5.8583e-03,  3.0586e-02, -9.9505e-03, -2.6503e-02,\n",
      "         1.1758e-02, -1.5898e-02,  2.3867e-03, -1.7946e-02,  3.3542e-03,\n",
      "        -1.5155e-02, -2.4473e-02,  1.0490e-02,  8.0267e-04, -7.8474e-03,\n",
      "        -1.7647e-02, -1.5782e-02, -3.9462e-03,  1.9148e-06, -5.3912e-03,\n",
      "         7.5356e-03, -1.1498e-02, -7.6340e-03,  2.3067e-02, -2.1039e-02,\n",
      "         2.6000e-02, -2.0384e-02, -1.4138e-02, -1.5352e-02, -2.8841e-02,\n",
      "        -2.6098e-02, -3.0169e-02,  8.1464e-03, -1.9418e-02,  1.2544e-02,\n",
      "         2.1124e-04,  2.6516e-02, -2.8513e-02, -2.2892e-02, -2.4692e-02,\n",
      "        -2.7929e-02,  1.1589e-02])\n",
      "latent.layers.1.hid_2sigma.weight:\n",
      "Initial weights: tensor([[ 0.0142, -0.0124, -0.0062,  ..., -0.0070, -0.0288,  0.0277],\n",
      "        [ 0.0078,  0.0259, -0.0077,  ..., -0.0280,  0.0160,  0.0235],\n",
      "        [ 0.0060, -0.0134, -0.0104,  ...,  0.0124,  0.0003, -0.0121],\n",
      "        ...,\n",
      "        [-0.0169,  0.0170,  0.0104,  ...,  0.0128, -0.0048,  0.0183],\n",
      "        [ 0.0201,  0.0209,  0.0279,  ...,  0.0027, -0.0167,  0.0213],\n",
      "        [-0.0188, -0.0212,  0.0146,  ...,  0.0146, -0.0123,  0.0262]])\n",
      "latent.layers.1.hid_2sigma.bias:\n",
      "Initial weights: tensor([ 0.0287, -0.0158, -0.0298,  0.0276,  0.0167,  0.0205, -0.0239,  0.0290,\n",
      "         0.0034, -0.0159,  0.0055,  0.0302,  0.0042, -0.0235,  0.0103,  0.0061,\n",
      "        -0.0012, -0.0178,  0.0131,  0.0258,  0.0106, -0.0252,  0.0285,  0.0096,\n",
      "        -0.0020,  0.0144, -0.0040,  0.0266,  0.0135, -0.0138,  0.0190, -0.0310,\n",
      "         0.0059, -0.0165,  0.0166,  0.0245, -0.0055, -0.0009,  0.0038,  0.0310,\n",
      "         0.0040,  0.0111, -0.0165,  0.0056, -0.0210, -0.0118,  0.0070, -0.0026,\n",
      "        -0.0112, -0.0025,  0.0259, -0.0001,  0.0260,  0.0296,  0.0162, -0.0171,\n",
      "         0.0221, -0.0213,  0.0167, -0.0079,  0.0213,  0.0126,  0.0296, -0.0306,\n",
      "         0.0117,  0.0139, -0.0234,  0.0040,  0.0254, -0.0020,  0.0017,  0.0014,\n",
      "        -0.0207,  0.0249, -0.0269,  0.0200,  0.0193, -0.0252, -0.0139, -0.0018,\n",
      "        -0.0104,  0.0296,  0.0185,  0.0254, -0.0150,  0.0220,  0.0182, -0.0201,\n",
      "        -0.0078,  0.0184, -0.0221, -0.0123,  0.0262,  0.0155,  0.0268, -0.0196,\n",
      "         0.0312, -0.0040,  0.0071,  0.0206, -0.0291,  0.0181, -0.0260, -0.0271,\n",
      "        -0.0247, -0.0140,  0.0056,  0.0224,  0.0010, -0.0049,  0.0036, -0.0113,\n",
      "        -0.0292, -0.0269,  0.0205, -0.0138,  0.0308,  0.0193, -0.0161,  0.0263,\n",
      "         0.0032,  0.0301, -0.0278,  0.0003,  0.0083, -0.0285, -0.0057, -0.0088,\n",
      "        -0.0141,  0.0260, -0.0256,  0.0061, -0.0312, -0.0084,  0.0161,  0.0222,\n",
      "         0.0278, -0.0056,  0.0170,  0.0236, -0.0029,  0.0005, -0.0037,  0.0049,\n",
      "         0.0109,  0.0217, -0.0170, -0.0309,  0.0057, -0.0040,  0.0064,  0.0113,\n",
      "         0.0196, -0.0185, -0.0030,  0.0173, -0.0192,  0.0103,  0.0183,  0.0163,\n",
      "        -0.0147, -0.0217, -0.0041, -0.0181, -0.0234, -0.0210,  0.0131,  0.0304,\n",
      "        -0.0265,  0.0234, -0.0228, -0.0245,  0.0086, -0.0054,  0.0081,  0.0003,\n",
      "         0.0051,  0.0028, -0.0218,  0.0243,  0.0086,  0.0264,  0.0012,  0.0006,\n",
      "        -0.0025, -0.0008,  0.0300,  0.0165,  0.0075, -0.0187, -0.0208,  0.0025,\n",
      "         0.0040,  0.0175, -0.0289, -0.0292,  0.0304, -0.0069,  0.0217,  0.0256,\n",
      "         0.0227, -0.0268,  0.0017, -0.0246, -0.0034, -0.0202, -0.0304, -0.0140,\n",
      "         0.0196,  0.0065,  0.0083,  0.0226, -0.0256,  0.0110,  0.0061,  0.0220,\n",
      "         0.0185, -0.0268,  0.0297, -0.0137, -0.0008, -0.0066,  0.0301, -0.0292,\n",
      "         0.0263, -0.0304,  0.0273, -0.0056, -0.0168, -0.0212, -0.0220,  0.0046,\n",
      "        -0.0271,  0.0006,  0.0243,  0.0066, -0.0126,  0.0016, -0.0037,  0.0023,\n",
      "        -0.0216, -0.0168, -0.0007,  0.0018,  0.0007,  0.0077,  0.0148,  0.0175,\n",
      "         0.0089,  0.0018, -0.0179,  0.0141,  0.0005, -0.0114, -0.0212, -0.0109,\n",
      "        -0.0217, -0.0163,  0.0047,  0.0312,  0.0093, -0.0100,  0.0042, -0.0263,\n",
      "        -0.0145,  0.0148,  0.0246,  0.0289, -0.0177,  0.0169,  0.0145, -0.0002,\n",
      "        -0.0020, -0.0154, -0.0072, -0.0122, -0.0187,  0.0045, -0.0204,  0.0135,\n",
      "        -0.0074, -0.0182,  0.0270, -0.0167,  0.0048, -0.0177,  0.0032, -0.0219,\n",
      "         0.0188,  0.0114,  0.0087, -0.0115,  0.0272, -0.0073,  0.0185, -0.0235,\n",
      "         0.0053, -0.0183, -0.0007,  0.0307,  0.0262, -0.0171,  0.0070, -0.0016,\n",
      "        -0.0133, -0.0066,  0.0051,  0.0104, -0.0177, -0.0026,  0.0187,  0.0309,\n",
      "        -0.0021, -0.0018,  0.0167,  0.0266, -0.0278,  0.0084,  0.0035, -0.0027,\n",
      "         0.0094,  0.0307, -0.0142, -0.0013,  0.0177, -0.0066, -0.0097,  0.0006,\n",
      "        -0.0308,  0.0093,  0.0185,  0.0137, -0.0125, -0.0039, -0.0202,  0.0212,\n",
      "         0.0039,  0.0010, -0.0274,  0.0243,  0.0240, -0.0194,  0.0005, -0.0165,\n",
      "        -0.0033,  0.0302,  0.0229,  0.0233, -0.0009, -0.0087,  0.0205, -0.0161,\n",
      "        -0.0078,  0.0150,  0.0154, -0.0146,  0.0098, -0.0017,  0.0024,  0.0283,\n",
      "        -0.0043, -0.0265, -0.0290, -0.0285,  0.0172, -0.0025, -0.0200,  0.0201,\n",
      "        -0.0009,  0.0232,  0.0285,  0.0051,  0.0260, -0.0309, -0.0149, -0.0309,\n",
      "         0.0080, -0.0225,  0.0245,  0.0079,  0.0218, -0.0292,  0.0113,  0.0014,\n",
      "         0.0176, -0.0001, -0.0046,  0.0077, -0.0022, -0.0078, -0.0055, -0.0135,\n",
      "        -0.0293,  0.0080,  0.0299,  0.0030,  0.0083,  0.0007,  0.0298,  0.0180,\n",
      "         0.0132,  0.0305, -0.0020,  0.0205,  0.0261,  0.0009,  0.0279,  0.0023,\n",
      "        -0.0246,  0.0124,  0.0279, -0.0067, -0.0124,  0.0004,  0.0054, -0.0310,\n",
      "         0.0130, -0.0036,  0.0151, -0.0166, -0.0279,  0.0273, -0.0116,  0.0159,\n",
      "         0.0172,  0.0080, -0.0191, -0.0287,  0.0095, -0.0070,  0.0247, -0.0179,\n",
      "         0.0077, -0.0072, -0.0228, -0.0122, -0.0016,  0.0228, -0.0301,  0.0278,\n",
      "        -0.0049,  0.0200,  0.0034, -0.0200, -0.0235,  0.0108,  0.0119, -0.0144,\n",
      "        -0.0012,  0.0285, -0.0312,  0.0203, -0.0148, -0.0133,  0.0177, -0.0309,\n",
      "         0.0247, -0.0076,  0.0055, -0.0287, -0.0120,  0.0154,  0.0139, -0.0312,\n",
      "         0.0296, -0.0101, -0.0095, -0.0175, -0.0185, -0.0087,  0.0229, -0.0284,\n",
      "        -0.0076, -0.0174,  0.0281, -0.0142, -0.0194,  0.0012, -0.0130, -0.0151,\n",
      "        -0.0047,  0.0060, -0.0232, -0.0310,  0.0283, -0.0220, -0.0002, -0.0136,\n",
      "        -0.0034, -0.0077,  0.0092,  0.0050, -0.0111,  0.0310,  0.0135,  0.0217,\n",
      "        -0.0181,  0.0129, -0.0234,  0.0012, -0.0080,  0.0173, -0.0015,  0.0054,\n",
      "         0.0050, -0.0289, -0.0223, -0.0215, -0.0266, -0.0123, -0.0220, -0.0211])\n",
      "head.mlp.1.weight:\n",
      "Initial weights: tensor([[-0.0090, -0.0159,  0.0274,  ...,  0.0411, -0.0169, -0.0056],\n",
      "        [-0.0346, -0.0336,  0.0034,  ...,  0.0200, -0.0092, -0.0044],\n",
      "        [ 0.0427,  0.0068, -0.0332,  ...,  0.0085, -0.0097,  0.0336],\n",
      "        ...,\n",
      "        [ 0.0334,  0.0411,  0.0256,  ...,  0.0014,  0.0201, -0.0087],\n",
      "        [-0.0207, -0.0345, -0.0276,  ...,  0.0433, -0.0341, -0.0197],\n",
      "        [-0.0009,  0.0088, -0.0334,  ...,  0.0398,  0.0273,  0.0145]])\n",
      "head.mlp.1.bias:\n",
      "Initial weights: tensor([-0.0345,  0.0339,  0.0230,  ...,  0.0322, -0.0032, -0.0091])\n",
      "head.mlp.4.weight:\n",
      "Initial weights: tensor([[ 0.0118, -0.0117, -0.0146,  ...,  0.0106, -0.0271, -0.0136],\n",
      "        [-0.0038,  0.0029,  0.0311,  ...,  0.0101, -0.0068, -0.0310],\n",
      "        [ 0.0203,  0.0055, -0.0236,  ..., -0.0061, -0.0082,  0.0181],\n",
      "        ...,\n",
      "        [-0.0086,  0.0305,  0.0079,  ..., -0.0042,  0.0065,  0.0146],\n",
      "        [ 0.0132,  0.0292,  0.0280,  ...,  0.0268, -0.0005,  0.0008],\n",
      "        [ 0.0226,  0.0122,  0.0262,  ..., -0.0181,  0.0132, -0.0128]])\n",
      "head.mlp.4.bias:\n",
      "Initial weights: tensor([ 0.0177,  0.0117, -0.0099,  ..., -0.0093, -0.0182, -0.0277])\n",
      "pre_latent_norm.norm.weight:\n",
      "Initial weights: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "pre_latent_norm.norm.bias:\n",
      "Initial weights: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "pipeline = SSLPipeline(pretrain_prefix=PRETRAIN_VERSION, # Specify the pretrain checkpoint to load\n",
    "                                      overwrite_config=model_config,  # This is for overwriting part of the pretrain config\n",
    "                                      pretrain_directory='../ckpt')\n",
    "model = pipeline.model\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}:\")\n",
    "        print(\"Initial weights:\", param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589e1d6d-f877-4f56-af6a-a972928fc7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 86168494\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Total number of parameters in the model: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2fa013-318f-49de-805d-766d1216e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 86168494\n"
     ]
    }
   ],
   "source": [
    "model\n",
    "\n",
    "# Assuming you have the OmicsFormer model defined as `model`\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7d4a9",
   "metadata": {},
   "source": [
    "Next, employ the `fit` function to fine-tune the model on your downstream dataset. This dataset should be in the form of an AnnData object, where `.X` is a csr_matrix. See previous section for more details.\n",
    "\n",
    "Typically, a dataset containing approximately 20,000 cells can be trained in under 10 minutes using a V100 GPU card, with an expected GPU memory consumption of around 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b199d53-9796-4950-beb4-b06e3c5adaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: AnnData object with n_obs × n_vars = 77105 × 407\n",
      "    obs: 'fov', 'volume', 'center_x', 'center_y', 'min_x', 'max_x', 'min_y', 'max_y', 'n_genes', 'batch', 'x_FOV_px', 'y_FOV_px', 'platform', 'Sample', 'Type', 'split'\n",
      "    var: 'n_cells-0', 'n_cells-1', 'ENSG-1'\n",
      "    obsm: 'truth'\n"
     ]
    }
   ],
   "source": [
    "print('train_data:', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b0b9cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, 407 genes remain.\n",
      "!!!!!!len(batch_gene_list[4]!!!!!!!!!!!!!!!!!!!!!!!): 307\n",
      "!!!!!!len(self.gene_list!!!!!!!!!!!!!!!!!!!!!!!): 407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [407] at index 0 does not match the shape of the indexed tensor [226, 19374] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# An AnnData object\u001b[39;49;00m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The config dictionary we created previously, optional\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit_field\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#  Specify a column in .obs that contains split information\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalid_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_gene_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_gene_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify genes that are measured in each batch, see previous section for more details\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cellplm/lib/python3.9/site-packages/CellPLM/pipeline/ssl.py:149\u001b[0m, in \u001b[0;36mSSLPipeline.fit\u001b[0;34m(self, adata, train_config, split_field, train_split, valid_split, covariate_fields, label_fields, batch_gene_list, ensembl_auto_conversion, device)\u001b[0m\n\u001b[1;32m    142\u001b[0m x_dict \u001b[38;5;241m=\u001b[39m XDict(input_dict)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# print('len(data_dict[gene_list]):', len(data_dict['gene_list']))\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# print('len(data_dict[gene_mask]):', len(data_dict['gene_mask']))\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# print('len(x_dict[gene_mask]):', len(x_dict['gene_mask']))\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# print('data_dict[gene_list]:', data_dict['gene_list'])\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# print('before entering the model x_dict:', x_dict)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# print('before entering the model data_dict:', data_dict)\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m out_dict, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgene_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    151\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cellplm/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cellplm/lib/python3.9/site-packages/CellPLM/model/cellformer.py:130\u001b[0m, in \u001b[0;36mOmicsFormer.forward\u001b[0;34m(self, x_dict, input_gene_list, d_iter)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m         out_dict, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         out_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m latent_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(latent_loss) \u001b[38;5;28;01melse\u001b[39;00m latent_loss\n\u001b[1;32m    132\u001b[0m         out_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/cellplm/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cellplm/lib/python3.9/site-packages/CellPLM/head/downstream.py:105\u001b[0m, in \u001b[0;36mImputationHead.forward\u001b[0;34m(self, x_dict)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict):\n\u001b[0;32m--> 105\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgene_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    106\u001b[0m     y \u001b[38;5;241m=\u001b[39m x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][:, x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    107\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse_loss(pred, y)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [407] at index 0 does not match the shape of the indexed tensor [226, 19374] at index 1"
     ]
    }
   ],
   "source": [
    "pipeline.fit(train_data, # An AnnData object\n",
    "            pipeline_config, # The config dictionary we created previously, optional\n",
    "            split_field = 'split', #  Specify a column in .obs that contains split information\n",
    "            train_split = 'train',\n",
    "            valid_split = 'valid',\n",
    "            batch_gene_list = batch_gene_list, # Specify genes that are measured in each batch, see previous section for more details\n",
    "            device = DEVICE,\n",
    "            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f80ee",
   "metadata": {},
   "source": [
    "## Inference and evaluation\n",
    "Once the pipeline has been fitted to the downstream datasets, performing inference or evaluation on new datasets can be easily accomplished using the built-in `predict` and `score` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(\n",
    "        query_data, # An AnnData object\n",
    "        pipeline_config, # The config dictionary we created previously, optional\n",
    "        device = DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(\n",
    "                query_data, # An AnnData object\n",
    "                evaluation_config = {'target_genes': target_genes}, # The config dictionary we created previously, optional\n",
    "                label_fields = ['truth'], # A field in .obsm that stores the ground-truth for evaluation\n",
    "                device = DEVICE,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15daa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#untrained\n",
    "{'mse': 0.1641050273599103,\n",
    " 'rmse': 0.4326375951244845,\n",
    " 'mae': 0.2050452692620456,\n",
    " 'corr': 0.3440739825367928,\n",
    " 'cos': 0.4543862997740507}\n",
    "\n",
    "#pretrained\n",
    "\n",
    "{'mse': 0.14237468457315117,\n",
    " 'rmse': 0.4044635064227159,\n",
    " 'mae': 0.19642182688228785,\n",
    " 'corr': 0.3790783796971664,\n",
    " 'cos': 0.4802470280230045}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37ef25-7f9e-4870-8442-99cb9433da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "out_dict = {'pred': torch.tensor([0.0001, 1.0, 10.0, 100.0])}\n",
    "truth = torch.tensor([0.0002, 1.1, 9.0, 95.0])\n",
    "\n",
    "# Applying log1p\n",
    "log_pred = torch.log1p(out_dict['pred'])\n",
    "log_truth = torch.log1p(truth)\n",
    "\n",
    "print(\"Log-transformed predictions:\", log_pred)\n",
    "print(\"Log-transformed true values:\", log_truth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18d2f5-9c35-45f4-83ec-bc6dd11629bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
